{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24261951\\AppData\\Local\\Temp\\ipykernel_17952\\1550422037.py:9: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('scripts')\n",
    "from TuneClass import *\n",
    "import TuneClass\n",
    "#import Executable\n",
    "import Envelope\n",
    "#from Executable import Tune\n",
    "#from Executable import main\n",
    "import imp\n",
    "#imp.reload(dbreader)\n",
    "#imp.reload(Executable)\n",
    "imp.reload(TuneClass)\n",
    "imp.reload(Envelope)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\24261951\\Documents\\Projects\\.venv\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import fcwt\n",
    "from pydub import AudioSegment\n",
    "import scipy.io.wavfile as wav  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tune:\n",
    "    def __init__(self, X=None, T=None, R=None, M=None, L=None, key=None, abc=\"\"):\n",
    "        self.X = X  # Reference number\n",
    "        self.T = T  # Title\n",
    "        self.R = R  # Rhythm type\n",
    "        self.M = M  # Meter\n",
    "        self.L = L  # Default note length\n",
    "        self.K = key  # Key signature\n",
    "        self.abc = abc  # Full ABC notation\n",
    "        self.notes = []  # Placeholder for extracted notes\n",
    "\n",
    "    def extract_notes(self):\n",
    "        \"\"\" Extracts the note sequence from ABC notation. \"\"\"\n",
    "        self.notes = abc_to_notestring(self.abc)\n",
    "        self.notes = keychanger(self,self.key)\n",
    "        self.tones = notes_to_semitones2(self,universal_encoder)  # Update after ABC is fully set\n",
    "        self.waveform = np.float32(constructor(self,'piano',.15))\n",
    "    def __str__(self):\n",
    "        return f\"X: {self.X}, T: {self.T}, R: {self.R}, M: {self.M}, L: {self.L}, K: {self.K}, Notes: {self.notes[:50]}...\"\n",
    "\n",
    "    def play(self,duration = .125,instrument='piano'):\n",
    "        waveform = constructor(self,instrument,duration)\n",
    "        waveform = np.float32(waveform)\n",
    "        sd.play(waveform,samplerate=8000)\n",
    "        sd.wait()\n",
    "\n",
    "# Read the file and parse entries\n",
    "tunes = []\n",
    "with open(\"reels.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    current_tune = None  # Holds the current tune being parsed\n",
    "\n",
    "    for line in file:\n",
    "        line = line.strip()  # Remove leading/trailing whitespace\n",
    "        \n",
    "        if not line:  # Empty line indicates a new tune might be starting\n",
    "            if current_tune:  # If there's an active tune, store it\n",
    "                tunes.append(current_tune)\n",
    "            current_tune = None  # Reset for the next tune\n",
    "            continue\n",
    "\n",
    "        if line.startswith(\"X:\"):  # Start of a new tune\n",
    "            if current_tune:  # Store the previous tune before starting a new one\n",
    "                tunes.append(current_tune)\n",
    "            current_tune = Tune(X=line[2:].strip())  # Create new tune instance\n",
    "        elif line.startswith(\"T:\") and current_tune:\n",
    "            current_tune.T = line[2:].strip()\n",
    "        elif line.startswith(\"R:\") and current_tune:\n",
    "            current_tune.R = line[2:].strip()\n",
    "        elif line.startswith(\"M:\") and current_tune:\n",
    "            current_tune.M = line[2:].strip()\n",
    "        elif line.startswith(\"L:\") and current_tune:\n",
    "            current_tune.L = line[2:].strip()\n",
    "        elif line.startswith(\"K:\") and current_tune:\n",
    "            current_tune.key = line[2:].strip()\n",
    "        elif current_tune and not re.match(r\"^[SCZ]:\", line):  # All other lines are part of the ABC notation\n",
    "            current_tune.abc += line + \"\\n\"\n",
    "\n",
    "    # Append the last parsed tune (if any)\n",
    "    if current_tune:\n",
    "        current_tune.extract_notes()\n",
    "        tunes.append(current_tune)\n",
    "for i in tunes:\n",
    "    i.extract_notes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image editing\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def reduce_resolution(image, new_shape):\n",
    "    # Compute magnitude and phase\n",
    "    magnitude = np.abs(image)\n",
    "    phase = np.angle(image)\n",
    "    \n",
    "    # Resize using OpenCV\n",
    "    magnitude_resized = cv2.resize(magnitude, new_shape, interpolation=cv2.INTER_AREA)  # Best for downscaling\n",
    "    phase_resized = cv2.resize(phase, new_shape, interpolation=cv2.INTER_LINEAR)  # Phase needs smooth interpolation\n",
    "    \n",
    "    # Reconstruct the complex image\n",
    "    resized_image = magnitude_resized * np.exp(1j * phase_resized)\n",
    "    \n",
    "    return resized_image\n",
    "\n",
    "def increase_contrast_complex(image, alpha=2.0, beta=0):\n",
    "    \"\"\"\n",
    "    Increase the contrast of a complex image array.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Complex numpy array (with real and imaginary parts)\n",
    "    - alpha: Contrast control (1.0 means no change, >1.0 increases contrast)\n",
    "    - beta: Brightness control (0 means no change)\n",
    "    \n",
    "    Returns:\n",
    "    - contrasted_image: The complex image with enhanced contrast\n",
    "    \"\"\"\n",
    "    # Separate magnitude and phase\n",
    "    magnitude = np.abs(image)\n",
    "    phase = np.angle(image)\n",
    "    \n",
    "    # Apply contrast enhancement to magnitude\n",
    "    magnitude_contrasted = np.clip(alpha * magnitude + beta, 0, 1)\n",
    "    \n",
    "    # Reconstruct the complex image with the enhanced magnitude\n",
    "    contrasted_image = magnitude_contrasted * np.exp(1j * phase)\n",
    "    \n",
    "    return contrasted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform, wavelet coherence, transform plot\n",
    "import numpy as np\n",
    "from pycwt import wct\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter, convolve1d,zoom\n",
    "import scipy.io.wavfile as wav\n",
    "from PIL import Image, ImageEnhance\n",
    "# Compute the wavelet coherence\n",
    "def gamma_correction_complex(image, gamma=.35,clip_point = .5):\n",
    "    # Compute magnitude and phase    \n",
    "    magnitude = np.abs(image)\n",
    "    phase = np.angle(image)\n",
    "    magnitude_max = np.max(magnitude)\n",
    "    if magnitude_max > 0:\n",
    "        magnitude = magnitude / magnitude_max  # Normalize to [0,1]\n",
    "    magnitude = np.clip(magnitude*4,clip_point,1)\n",
    "    # Apply gamma correction to the magnitude\n",
    "    magnitude_corrected = np.power(magnitude, gamma)\n",
    "    blur_strength = 2\n",
    "    blur_kernel = np.ones(blur_strength) / blur_strength  # 1D blur kernel\n",
    "    magnitude_corrected = convolve1d(magnitude_corrected, blur_kernel, axis=1, mode='reflect')\n",
    "\n",
    "    # Reconstruct the complex image\n",
    "    corrected_image = magnitude_corrected * np.exp(1j * phase)\n",
    "    return corrected_image\n",
    "\n",
    "def transform(signal1,frame_rate,clip_point,correction= True):\n",
    "    highest = 5000\n",
    "    lowest = 200\n",
    "    nfreqs = 100\n",
    "    freqs, coeffs1 = fcwt.cwt(signal1,frame_rate,lowest,highest,nfreqs,nthreads = 4,scaling='log')\n",
    "    coeffs1 = reduce_resolution(coeffs1, (400,200))\n",
    "    if correction:\n",
    "        coeffs1 = gamma_correction_complex(coeffs1,clip_point = clip_point)\n",
    "    return coeffs1,freqs\n",
    "\n",
    "def transform_plot(signal,frame_rate,clip_point,correction = True):\n",
    "    coeffs1,freqs = transform(signal,frame_rate,clip_point,correction)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(np.abs(coeffs1), aspect='auto', extent=[0, len(signal)/frame_rate, freqs[-1], freqs[0]], cmap='jet')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('Wavelet Transform Magnitude')\n",
    "    plt.show()\n",
    "    return coeffs1,freqs\n",
    "\n",
    "def wavelet_coherence(coeffs1,coeffs2,frame_rate,freqs):\n",
    "\n",
    "    # Compute coherence\n",
    "    S1 = np.abs(coeffs1) ** 2\n",
    "    S2 = np.abs(coeffs2) ** 2\n",
    "    S12 = coeffs1 * np.conj(coeffs2)\n",
    "    \n",
    "    # Smooth spectra and cross-spectrum\n",
    "    def smooth(data, sigma=(2,2), mode='nearest'):\n",
    "        return data\n",
    "        #ensure only horizontal smoothing\n",
    "        return gaussian_filter(data, sigma=sigma, mode=mode)\n",
    "\n",
    "    S1_smoothed = smooth(S1)\n",
    "    S2_smoothed = smooth(S2)\n",
    "    S12_smoothed = smooth(np.abs(S12) ** 2)\n",
    "\n",
    "    # Coherence calculation\n",
    "    #coherence = S12_smoothed / (((S1_smoothed) **2) * ((S2_smoothed)**2))\n",
    "    coherence = S12_smoothed / (np.sqrt(S1_smoothed) * np.sqrt(S2_smoothed))\n",
    "\n",
    "    return coherence, freqs \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_signal_length(signal1, signal2, pad_value=0):\n",
    "    \"\"\"\n",
    "    Adjusts signal2 to match the length of signal1 by either padding or trimming.\n",
    "    \n",
    "    Parameters:\n",
    "    - signal1 (numpy array): The reference signal.\n",
    "    - signal2 (numpy array): The signal to adjust.\n",
    "    - pad_value (int or float, optional): The value to use for padding. Default is 0.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array: signal2, modified to match the length of signal1.\n",
    "    \"\"\"\n",
    "    len1 = len(signal1)\n",
    "    len2 = len(signal2)\n",
    "    \n",
    "    if len2 > len1:\n",
    "        # Trim signal2 if it's longer\n",
    "        return signal2[:len1]\n",
    "    elif len2 < len1:\n",
    "        # Pad signal2 if it's shorter\n",
    "        padding = np.full((len1 - len2,), pad_value)  # Create padding array\n",
    "        return np.concatenate((signal2, padding))\n",
    "    else:\n",
    "        # Already the same length\n",
    "        return signal2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_signals(tune,recording,frame_rate,plot=True):\n",
    "    \n",
    "    audio_data1 = np.array(recording, dtype=np.float32)\n",
    "    audio_data1 -= np.mean(audio_data1)\n",
    "    # 2. Divide by the maximum absolute value to scale between -1 and 1\n",
    "    max1 = np.max(np.abs(audio_data1))\n",
    "    if max1 > 0:\n",
    "        data1 = audio_data1 / max1\n",
    "    data2 = tune.waveform\n",
    "    audio_data2 = np.array(data2, dtype=np.float32)\n",
    "    audio_data2 -= np.mean(audio_data2)\n",
    "    # 2. Divide by the maximum absolute value to scale between -1 and 1\n",
    "    max2 = np.max(np.abs(audio_data2))\n",
    "    if max2 > 0:\n",
    "        data2 = audio_data2 / max2\n",
    "        \n",
    "    data2 = match_signal_length(data1, data2)\n",
    "    coeffs1,freqs = transform(data1,frame_rate)\n",
    "    coeffs2,_ = transform(data2,frame_rate)\n",
    "    coherence, freqs = wavelet_coherence(coeffs1,coeffs2, frame_rate,freqs)\n",
    "    extent=[0, len(data1)/frame_rate, len(freqs),0]\n",
    "\n",
    "    coherence = np.clip(coherence,.7,None)\n",
    "    y_values = np.logspace(np.log10(freqs[0]), np.log10(freqs[-1]), len(freqs))\n",
    "    subsampled_indices = np.linspace(0, len(y_values) - 1, 20, dtype=int)\n",
    "    subsampled_y_values = y_values[subsampled_indices]\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(coherence, aspect='auto', extent=extent,cmap = 'jet')\n",
    "        plt.yticks(ticks=subsampled_indices, labels=[f\"{y:.1f}\" for y in subsampled_y_values])\n",
    "        plt.xticks(np.linspace(0, len(data1)/frame_rate, 5))\n",
    "        plt.colorbar(label='Coherence')\n",
    "        plt.ylabel('Scale (Frequency)')\n",
    "        plt.xlabel('Time (seconds)')\n",
    "        plt.title(f'Wavelet Coherence: {tune.T}')\n",
    "        plt.show()\n",
    "    return coeffs1,coeffs2,freqs,coherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 8000\n",
    "def create_transforms(tunes):\n",
    "    for i in tunes:\n",
    "        audio_data1 = np.array(i.waveform, dtype=np.float32)\n",
    "        audio_data1 -= np.mean(audio_data1)\n",
    "        # 2. Divide by the maximum absolute value to scale between -1 and 1\n",
    "        max1 = np.max(np.abs(audio_data1))\n",
    "        if max1 > 0:\n",
    "            data1 = audio_data1 / max1\n",
    "        i.transform = transform(data1,frame_rate,clip_point = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence plot\n",
    "from IPython.display import display\n",
    "def coherence_plot(tune,recording_transform,frame_rate,plot=True): \n",
    "    # should be able to take in a tune object and a recording object and\n",
    "    # efficiently compute the coherence between the two waveforms. without having \n",
    "    # to recompute the wavelet transform for the tune object\n",
    "    coeffs1 = tune.transform[0]\n",
    "    coeffs2 = recording_transform\n",
    "    freqs = tune.transform[1]\n",
    "    coherence, freqs = wavelet_coherence(coeffs1,coeffs2, frame_rate,freqs)\n",
    "    extent=[0, len(tune.waveform)/frame_rate, len(freqs),0]\n",
    "\n",
    "    coherence = np.where(coherence<.1,0,coherence)\n",
    "    max1 = np.max(np.abs(coherence))\n",
    "    min1 = np.min(np.abs(coherence))\n",
    "    coherence = (coherence-min1) / (max1-min1)\n",
    "    y_values = np.logspace(np.log10(freqs[0]), np.log10(freqs[-1]), len(freqs))\n",
    "    subsampled_indices = np.linspace(0, len(y_values) - 1, 20, dtype=int)\n",
    "    subsampled_y_values = y_values[subsampled_indices]\n",
    "    plt_obj = None\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(coherence, aspect='auto', extent=extent,cmap = 'jet')\n",
    "        plt.yticks(ticks=subsampled_indices, labels=[f\"{y:.1f}\" for y in subsampled_y_values])\n",
    "        plt.xticks(np.linspace(0, len(tune.waveform)/frame_rate, 5))\n",
    "        plt.colorbar(label='Coherence')\n",
    "        plt.ylabel('Scale (Frequency)')\n",
    "        plt.xlabel('Time (seconds)')\n",
    "        plt.title(f'Wavelet Coherence: {tune.T}')\n",
    "        plt_obj = plt.gcf()\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "    return coeffs1,coeffs2,freqs,coherence,plt_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_using_cross_correlation(ref_signal, target_signal, sr):\n",
    "    \"\"\"\n",
    "    Aligns target_signal to ref_signal using cross-correlation.\n",
    "    \n",
    "    Parameters:\n",
    "        ref_signal (numpy array): The reference audio signal.\n",
    "        target_signal (numpy array): The misaligned audio signal.\n",
    "        sr (int): Sample rate of the audio.\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: The aligned target signal.\n",
    "    \"\"\"\n",
    "    # Compute cross-correlation\n",
    "    corr = np.correlate(target_signal, ref_signal, mode=\"full\")\n",
    "    lag = np.argmax(corr) - len(ref_signal) + 1  # Find the best shift\n",
    "\n",
    "    # Apply time shift (trim or pad)\n",
    "    if lag > 0:\n",
    "        aligned_target = target_signal[lag:]  # Cut off leading part\n",
    "    else:\n",
    "        aligned_target = np.pad(target_signal, (abs(lag), 0), mode=\"constant\")  # Pad with zeros\n",
    "\n",
    "    return aligned_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m4a to reel, trim silence\n",
    "AudioSegment.converter = \"C:/Users/24261951/Documents/Projects/ffmpeg-master-latest-win64-gpl/bin/ffmpeg.exe\"                  \n",
    "from pydub import utils, AudioSegment\n",
    "\n",
    "def get_prober_name():\n",
    "    return \"C:/Users/24261951/Documents/Projects/ffmpeg-master-latest-win64-gpl/bin/ffprobe.exe\"\n",
    "utils.get_prober_name = get_prober_name\n",
    "\n",
    "def trim_silence(signal, threshold):\n",
    "    \"\"\"\n",
    "    Removes all values before the signal first exceeds the threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    signal (numpy array): The input signal\n",
    "    threshold (float): The minimum amplitude to start keeping values\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: Trimmed signal starting from the first peak above the threshold\n",
    "    \"\"\"\n",
    "    # Find the first index where the signal exceeds the threshold\n",
    "    index = np.argmax(np.abs(signal) >= threshold)\n",
    "    \n",
    "    # Slice the signal from that index onwards\n",
    "    return signal[index:] if np.abs(signal[index]) >= threshold else np.array([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m4a to reel, wav to reel\n",
    "def m4a_to_reel(file):\n",
    "    audio = AudioSegment.from_file(file,format='m4a')\n",
    "    audio = audio.set_frame_rate(8000)\n",
    "    if audio.channels == 2:\n",
    "        samples = samples.reshape((-1, 2)).mean(axis=1)\n",
    "    audio = np.frombuffer(audio.raw_data,np.int16)\n",
    "    audio = (audio)/max(abs(audio))\n",
    "    audio = trim_silence(audio,0.5) # begin when playing starts\n",
    "    audio = match_signal_length(tunes[0].waveform,audio)\n",
    "    #audio = align_using_cross_correlation(tunes[1].waveform,audio,8000)\n",
    "    return audio\n",
    "\n",
    "def wav_to_reel(file):\n",
    "    audio = AudioSegment.from_file(file,format='wav')\n",
    "    audio = audio.set_frame_rate(8000)\n",
    "    if audio.channels == 2:\n",
    "        audio = audio.set_channels(1)\n",
    "    audio = np.frombuffer(audio.raw_data,np.int16)\n",
    "    audio = (audio)/max(abs(audio))\n",
    "    audio = trim_silence(audio,0.5) # begin when playing starts\n",
    "    audio = match_signal_length(tunes[0].waveform,audio)\n",
    "    #audio = align_using_cross_correlation(tunes[1].waveform,audio,8000)\n",
    "    return audio\n",
    "\n",
    "def wav_to_data(file,set_fr = False):\n",
    "    audio = AudioSegment.from_file(file,format='wav')\n",
    "    if set_fr:\n",
    "        audio = audio.set_frame_rate(8000)\n",
    "    if audio.channels == 2:\n",
    "        audio = audio.set_channels(1)\n",
    "    audio = np.frombuffer(audio.raw_data,np.int16)\n",
    "    audio = (audio)/max(abs(audio))\n",
    "    audio = trim_silence(audio,0.5) # begin when playing starts\n",
    "    #audio = align_using_cross_correlation(tunes[1].waveform,audio,8000)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots from png\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_subplots_from_png(directory):\n",
    "    # Get list of all .png files in the directory\n",
    "    png_files = [f for f in os.listdir(directory) if f.endswith('.png')]\n",
    "    \n",
    "    # Determine the number of rows and columns for the subplots\n",
    "    num_files = len(png_files)\n",
    "    cols = 6  # Number of columns in the subplot grid\n",
    "    rows = (num_files // cols) + (num_files % cols > 0)  # Calculate rows needed\n",
    "\n",
    "    # Create a figure and axes for the subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for ax, png_file in zip(axes, png_files):\n",
    "        img = Image.open(os.path.join(directory, png_file))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(png_file[:-4])\n",
    "        ax.axis('off')  # Hide the axes\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for ax in axes[num_files:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def workflow(recording,frame_rate = 8000,plot = False,save = False,title = 'unspecified title',clip_point = .1):\n",
    "    file = f'recordings/{recording}'\n",
    "    if file[-3:] == ('m4a'):\n",
    "        tune = m4a_to_reel(file)\n",
    "    elif file[-3:] == ('wav'):\n",
    "        tune = wav_to_reel(file)\n",
    "    elif type(recording) == np.ndarray or type(file) == list:\n",
    "        tune = recording\n",
    "        recording = title\n",
    "    else:\n",
    "        return \"Unsupported input format\"\n",
    "    sums = {}\n",
    "    tform = transform(tune,frame_rate,clip_point = clip_point)\n",
    "    if save:\n",
    "        output_dir = f'coherence_results/{recording}'\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)  # Remove only if it exists\n",
    "        os.makedirs(output_dir)\n",
    "    # Calculate the required number of rows\n",
    "    for n,i in enumerate(tunes):\n",
    "        _,_,_,coh,plt_obj = coherence_plot(i,tform[0],frame_rate,plot=True)\n",
    "        \n",
    "        if save:\n",
    "            plt_obj.savefig(f'coherence_results/{recording}/{i.T}.png')\n",
    "\n",
    "        sums[i.T] = np.where(coh<.5,0,coh).sum()\n",
    "    if plot:\n",
    "        create_subplots_from_png(f'coherence_results/{recording}/')\n",
    "\n",
    "    min_value = min(sums.values())\n",
    "\n",
    "    # Divide all values by the minimum\n",
    "    normalized_sums = {key: value - min_value for key, value in sums.items()}   \n",
    "    keys = list(normalized_sums.keys())\n",
    "    values = list(normalized_sums.values())\n",
    "    # Plot a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(keys, values)\n",
    "    plt.xlabel('Tune')\n",
    "    plt.xticks(rotation=90,fontsize=8)\n",
    "    plt.ylabel('Total Coherence')   \n",
    "    plt.title(f'Summed Coherence: Synthetics VS Recording {recording}')\n",
    "    if save:\n",
    "        plt.savefig(f'coherence_results/{recording}/_summed_coherence.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def generate_click(samplerate=44100):\n",
    "    \"\"\"Generates a short, percussive click sound.\"\"\"\n",
    "    duration = 0.1  # 10ms for sharp click\n",
    "    samples = int(samplerate * duration)\n",
    "\n",
    "    # White noise burst\n",
    "    noise = np.random.uniform(-1.0, 1.0, samples) * 0.5  \n",
    "\n",
    "    # Apply an envelope for fast decay\n",
    "    envelope = np.linspace(1, 0, samples)  # Fast fade-out\n",
    "    click_sound = noise * envelope\n",
    "\n",
    "    return click_sound.astype(np.float32), duration  # Return both sound & duration\n",
    "\n",
    "def generate_tone(frequency=1000, duration=0.1, samplerate=44100):\n",
    "    \"\"\"Generates a beep sound for count-in.\"\"\"\n",
    "    t = np.linspace(0, duration, int(samplerate * duration), endpoint=False)\n",
    "    tone = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
    "    return tone.astype(np.float32), duration  # Return both sound & duration\n",
    "\n",
    "def metronome_thread(bpm, num_clicks,count_in):\n",
    "    \"\"\"Plays a count-in (8 beeps) followed by exactly `num_clicks` metronome clicks.\"\"\"\n",
    "    global recording_active\n",
    "    beat_interval = 60.0 / bpm  # Exact time per beat (seconds)\n",
    "\n",
    "    beep_sound, beep_duration = generate_tone(frequency=1000, duration=0.1)  # Count-in sound\n",
    "    click_sound, click_duration = generate_click()  # Sharp click\n",
    "\n",
    "    with sd.OutputStream(samplerate=44100, channels=1) as stream:\n",
    "        start_time = time.monotonic()  # Start timing reference\n",
    "\n",
    "        # Count-in (8 beeps)\n",
    "        for _ in range(count_in):\n",
    "            if not recording_active:\n",
    "                return\n",
    "            stream.write(beep_sound)\n",
    "            next_beat_time = start_time + (_ + 1) * beat_interval\n",
    "            time.sleep(max(0, next_beat_time - time.monotonic()))  # Sync precisely\n",
    "\n",
    "        # Play exactly `num_clicks` metronome clicks\n",
    "        for i in range(num_clicks):\n",
    "            if not recording_active:\n",
    "                return\n",
    "            stream.write(click_sound)\n",
    "            next_beat_time = start_time + (i + 9) * beat_interval  # Continue timing after count-in\n",
    "            time.sleep(max(0, next_beat_time - time.monotonic()))  # Sync precisely\n",
    "\n",
    "def record_audio_with_metronome(bpm, num_clicks, samplerate,count_in):\n",
    "    \"\"\"Records audio while playing exactly `num_clicks` metronome clicks after a count-in.\"\"\"\n",
    "    global recording_active\n",
    "    recording_active = True\n",
    "\n",
    "    # Calculate recording duration (8 count-in beeps + num_clicks clicks)\n",
    "    duration = ((num_clicks + count_in) * 60) / bpm  \n",
    "\n",
    "    print(f\"Recording for {duration:.2f} seconds with metronome at {bpm} BPM...\")\n",
    "\n",
    "    # Start the metronome in a separate thread\n",
    "    metronome_threading = threading.Thread(target=metronome_thread, args=(bpm, num_clicks,count_in))\n",
    "    metronome_threading.start()\n",
    "    \n",
    "    time.sleep(0.1)  # Ensures metronome starts before recording\n",
    "\n",
    "    # Start recording\n",
    "    audio_data = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=2, dtype=np.int16)\n",
    "    sd.wait()  # Ensure the recording completes\n",
    "    audio_data = np.mean(audio_data, axis=1, dtype=np.int16)\n",
    "\n",
    "    recording_active = False  # Stop metronome after recording\n",
    "    metronome_threading.join()\n",
    "\n",
    "    # Save the recording\n",
    "    return audio_data\n",
    "    print(f\"Recording saved as {filename}\")\n",
    "\n",
    "# Example usage\n",
    "#bpm = 200\n",
    "#num_clicks = 65 # Play exactly 64 clicks\n",
    "#count_in = 9\n",
    "#record_audio_with_metronome(\"metronome_record2.wav\", bpm=bpm, num_clicks=num_clicks,count_in=count_in,samplerate=44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_transforms(tunes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(frame_rate = 8000,bpm = 200,num_clicks = 65,count_in = 9,save = False,plot = False):\n",
    "    audio_data1 = record_audio_with_metronome(bpm = bpm,\n",
    "                                             num_clicks = num_clicks,\n",
    "                                             samplerate=frame_rate,\n",
    "                                             count_in = count_in)\n",
    "    \n",
    "    # capture the corresponding segment of the tune\n",
    "    factor = frame_rate/8000\n",
    "    window_size = 154800 *factor  # Fixed window size\n",
    "    end_sample = int(factor*(-1 - 81)) # End of the window moves left each iteration - optimal\n",
    "    start_sample = int(factor*(end_sample - window_size + 1))  # Ensure the window remains 154800 samples\n",
    "\n",
    "    # Slice the audio data\n",
    "    audio_data = audio_data1[start_sample:end_sample + 1]  # +1 to include `end_sample`\n",
    "    filepath = 'unspecified title'\n",
    "\n",
    "    if save:\n",
    "\n",
    "        inp = input('Save recording? (y/n): ')\n",
    "\n",
    "        if inp.lower() == 'y':    \n",
    "            filepath = input('YourName Intstrument TuneTitle')\n",
    "            wav.write('recordings/'+filepath+'.wav',frame_rate,audio_data)\n",
    "            \n",
    "            # Specify the name of the directory to be created\n",
    "            directory = 'coherence_results/'+filepath\n",
    "\n",
    "            # Check if the directory already exists\n",
    "            if os.path.exists(directory):\n",
    "                response = input(f\"Results for '{directory}' already exists. Do you want to overwrite it? (y/n): \")\n",
    "                if response.lower() == 'y':\n",
    "                    shutil.rmtree(directory)  # Remove the existing directory\n",
    "                    os.makedirs(directory)  # Create a new directory\n",
    "                    print(f\"Directory '{directory}' overwritten successfully\")\n",
    "                else:\n",
    "                    print(\"Operation cancelled\")\n",
    "            else:\n",
    "                os.makedirs(directory)\n",
    "                print(f\"Directory '{directory}' created successfully\")\n",
    "            wav.write('coherence_results/'+filepath+'/raw_audio.wav',frame_rate,audio_data)\n",
    "\n",
    "    sums = workflow(audio_data,frame_rate,plot = plot,save=save,title=filepath)\n",
    "  \n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 22.20 seconds with metronome at 200 BPM...\n"
     ]
    },
    {
     "ename": "PortAudioError",
     "evalue": "Error querying device -1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sums \u001b[38;5;241m=\u001b[39m \u001b[43mrecord\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m, in \u001b[0;36mrecord\u001b[1;34m(frame_rate, bpm, num_clicks, count_in, save, plot)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrecord\u001b[39m(frame_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8000\u001b[39m,bpm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m,num_clicks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65\u001b[39m,count_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m,save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     audio_data1 \u001b[38;5;241m=\u001b[39m \u001b[43mrecord_audio_with_metronome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbpm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbpm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mnum_clicks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_clicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mcount_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcount_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# capture the corresponding segment of the tune\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     factor \u001b[38;5;241m=\u001b[39m frame_rate\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8000\u001b[39m\n",
      "Cell \u001b[1;32mIn[19], line 71\u001b[0m, in \u001b[0;36mrecord_audio_with_metronome\u001b[1;34m(bpm, num_clicks, samplerate, count_in)\u001b[0m\n\u001b[0;32m     68\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# Ensures metronome starts before recording\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Start recording\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m sd\u001b[38;5;241m.\u001b[39mwait()  \u001b[38;5;66;03m# Ensure the recording completes\u001b[39;00m\n\u001b[0;32m     73\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(audio_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint16)\n",
      "File \u001b[1;32mc:\\Users\\24261951\\Documents\\Projects\\.venv\\Lib\\site-packages\\sounddevice.py:279\u001b[0m, in \u001b[0;36mrec\u001b[1;34m(frames, samplerate, channels, dtype, out, mapping, blocking, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mread_indata(indata)\n\u001b[0;32m    277\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mcallback_exit()\n\u001b[1;32m--> 279\u001b[0m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mInputStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\24261951\\Documents\\Projects\\.venv\\Lib\\site-packages\\sounddevice.py:2626\u001b[0m, in \u001b[0;36m_CallbackContext.start_stream\u001b[1;34m(self, StreamClass, samplerate, channels, dtype, callback, blocking, **kwargs)\u001b[0m\n\u001b[0;32m   2623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m, StreamClass, samplerate, channels, dtype, callback,\n\u001b[0;32m   2624\u001b[0m                  blocking, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2625\u001b[0m     stop()  \u001b[38;5;66;03m# Stop previous playback/recording\u001b[39;00m\n\u001b[1;32m-> 2626\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[43mStreamClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2627\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2628\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2629\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2630\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinished_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinished_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2631\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m   2633\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _last_callback\n",
      "File \u001b[1;32mc:\\Users\\24261951\\Documents\\Projects\\.venv\\Lib\\site-packages\\sounddevice.py:1440\u001b[0m, in \u001b[0;36mInputStream.__init__\u001b[1;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1409\u001b[0m              device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, latency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1410\u001b[0m              extra_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, finished_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1411\u001b[0m              clip_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, never_drop_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1412\u001b[0m              prime_output_buffers_using_stream_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"PortAudio input stream (using NumPy).\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \n\u001b[0;32m   1415\u001b[0m \u001b[38;5;124;03m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \n\u001b[0;32m   1439\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1440\u001b[0m     \u001b[43m_StreamBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_remove_self\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\24261951\\Documents\\Projects\\.venv\\Lib\\site-packages\\sounddevice.py:828\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[1;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[0;32m    825\u001b[0m         samplerate \u001b[38;5;241m=\u001b[39m isamplerate\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize, samplerate \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m--> 828\u001b[0m         \u001b[43m_get_stream_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mextra_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mchannelCount\n",
      "File \u001b[1;32mc:\\Users\\24261951\\Documents\\Projects\\.venv\\Lib\\site-packages\\sounddevice.py:2708\u001b[0m, in \u001b[0;36m_get_stream_parameters\u001b[1;34m(kind, device, channels, dtype, latency, extra_settings, samplerate)\u001b[0m\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m samplerate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2706\u001b[0m     samplerate \u001b[38;5;241m=\u001b[39m default\u001b[38;5;241m.\u001b[39msamplerate\n\u001b[1;32m-> 2708\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mquery_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2710\u001b[0m     channels \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m kind \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_channels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\24261951\\Documents\\Projects\\.venv\\Lib\\site-packages\\sounddevice.py:572\u001b[0m, in \u001b[0;36mquery_devices\u001b[1;34m(device, kind)\u001b[0m\n\u001b[0;32m    570\u001b[0m info \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mPa_GetDeviceInfo(device)\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info:\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PortAudioError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError querying device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m info\u001b[38;5;241m.\u001b[39mstructVersion \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    574\u001b[0m name_bytes \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(info\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[1;31mPortAudioError\u001b[0m: Error querying device -1"
     ]
    }
   ],
   "source": [
    "sums = record(save=True,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsult = []\\nfor i in range(75,85):\\n    audio_data, sums = opt(x, i)\\n\\n    if \\'Banshee, The\\' in sums:\\n        sult.append((sums[\\'Banshee, The\\'],i))\\n        print(sult[-1])\\n    else:\\n        print(f\"\\'Banshee, The\\' not found in iteration {i}\")\\'\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimal var finder for the slice\n",
    "def opt(audio_data1, var, frame_rate=8000, bpm=200, num_clicks=65, count_in=9, save=False, plot=False):\n",
    "    window_size = 154800  # Fixed window size\n",
    "    end_sample = -1 - var  # End of the window moves left each iteration\n",
    "    start_sample = end_sample - window_size + 1  # Ensure the window remains 154800 samples\n",
    "\n",
    "    # Slice the audio data\n",
    "    audio_data = audio_data1[start_sample:end_sample + 1]  # +1 to include `end_sample`\n",
    "\n",
    "    if len(audio_data) != window_size:\n",
    "        raise ValueError(f\"Invalid slice at iteration {var}: start={start_sample}, end={end_sample}, length={len(audio_data)}\")\n",
    "\n",
    "    # Process with `work`\n",
    "    filepath = 'unspecified title'\n",
    "    sums = work(audio_data, frame_rate, plot=plot, save=save, title=filepath)\n",
    "\n",
    "    return audio_data, sums\n",
    "\n",
    "\n",
    "'''\n",
    "sult = []\n",
    "for i in range(75,85):\n",
    "    audio_data, sums = opt(x, i)\n",
    "\n",
    "    if 'Banshee, The' in sums:\n",
    "        sult.append((sums['Banshee, The'],i))\n",
    "        print(sult[-1])\n",
    "    else:\n",
    "        print(f\"'Banshee, The' not found in iteration {i}\")'\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
